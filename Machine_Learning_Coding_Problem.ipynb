{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Coding Problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRjW7uMdNQsn0kEB5NjX1f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phonism/notes/blob/master/Machine_Learning_Coding_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kmeans\n",
        "K-means clustering is an unsupervised machine learning algorithm that seeks to segment a dataset into groups based on the similarity of datapoints.\n",
        "\n",
        "The kMeans algorithm finds those k points (called centroids) that minimize the sum of squared errors.\n",
        "\n",
        "Thus, the Kmeans algorithm consists of the following steps:\n",
        "+ We initialize k centroids randomly.\n",
        "+ Then we calcuate the distance from each point to each centroid.\n",
        "+ Assign each point to their nearest centroid.\n",
        "+ Centroids are shifted to be the average value of the points belonging to it.If the centroids did not move, the algorithm is finished, else repeat.\n"
      ],
      "metadata": {
        "id": "x95aLfDxqUAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lEADrCj9qSJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "### Introduction\n",
        "Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog).\n",
        "\n",
        "The main type of linear regression is \n",
        "$$f(x)=w_{1}x1 + w_{2}x2 + w_{3}x3 + b$$\n",
        "which $w$ represents the weights, our model will try to learn. The variables $x1, x2,x3$ represent the input data's attributes. The variable $y$ represents our prediction.\n",
        "\n",
        "### **Making Predictions**\n",
        "Let's say our predict function outputs an estimate of ctr(click-through rate)  given our current weights and some attributes $x1,x2,x3$. \n",
        "```python\n",
        "def forward(self, x):\n",
        "    y_pred = np.matmul(x, self.w) + self.b\n",
        "    return y_pred\n",
        "```\n",
        "### **Cost Function**\n",
        "Now we need a cost function to audit how our model is performing. We use MSE function. \n",
        "$$MSE=\\frac{1}{2n}\\sum_{i=1}^{n}(y_i - (w_{1}x1_{i} + w_{2}x2_{i}+w_{3}x3_{i} + b))^2$$\n",
        "Our model will try to identify weight values that most reduce our cost function.\n",
        "```python\n",
        "def cost_function(self, x, y_true):\n",
        "    n = len(y_true)\n",
        "    y_pred = self.forward(x)\n",
        "    sq_error = (y_pred - y_true) ** 2\n",
        "    return 1.0 / (2 * n) * sq_error.sum()\n",
        "```\n",
        "\n",
        "### **Gradient Descent**\n",
        "To minimize MSE we use Gradient Descent to calculate the gradient of our cost function. Gradient descent consists of looking at the error that our weight currently gives us, using the derivative of the cost function to find the gradient (The slope of the cost function using our current weight), and then changing our weight to move in the direction opposite of the gradient. We need to move in the opposite direction of the gradient since the gradient points up the slope instead of down it, so we move in the opposite direction to try to decrease our error.\n",
        "\n",
        "Again using the Chain rule we can compute the gradient vector of partial derivatives describing the slope of the cost function for each weight.\n",
        "$$f^{'}(w_{1}) = -x1(y - (w_{1}x1 + w_{2}x2 + w_{3}x3 + b)$$\n",
        "$$f^{'}(w_{2}) = -x2(y - (w_{1}x1 + w_{2}x2 + w_{3}x3 + b)$$\n",
        "$$f^{'}(w_{3}) = -x3(y - (w_{1}x1 + w_{2}x2 + w_{3}x3 + b)$$\n",
        "```python\n",
        "def compute_gradients(self, x, y_true):\n",
        "    y_pred = self.forward(x)\n",
        "    difference = y_pred - y_true\n",
        "    gradient_b = np.mean(difference)\n",
        "    gradients_w = np.matmul(x.transpose(), difference)\n",
        "    gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
        "```\n",
        "\n",
        "### **Train**\n",
        "Training a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope of the cost function (gradient). Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost.\n",
        "```python\n",
        "def __init__(self):\n",
        "    self.w = np.random.rand(feature_num)\n",
        "    self.b = np.random.rand(1)\n",
        "\n",
        "def update_weightes(self, x, y_true, learning_rate):\n",
        "    gradients_w, gradient_b = self.compute_gradients(x, y_true)\n",
        "    self.w = self.w - learning_rate * gradients_w\n",
        "    self.b = self.b - learning_rate * gradient_b\n",
        "\n",
        "def train(self, x, y):\n",
        "    epoch_num = 1000\n",
        "    learning_rate = 0.01\n",
        "    for epoch in range(epoch_num):\n",
        "        model.update_weightes(x, y, learning_rate)\n",
        "```\n",
        "\n",
        "### **Normalize**\n",
        "As the number of features grows, calculating gradient takes longer to compute, and the gradient may be very huge and we may meet some overflow err. We can speed this up by “normalizing” our input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Our goal now will be to normalize our features so they are all in the range -1 to 1.\n",
        "```python\n",
        "def normalize(self, x):\n",
        "    for feature in x.T:\n",
        "        fmean = np.mean(feature)\n",
        "        frange = np.amax(feature) - np.amin(feature)\n",
        "        feature -= fmean\n",
        "        feature /= frange\n",
        "    return x\n",
        "```"
      ],
      "metadata": {
        "id": "cixrSV0Gtzqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_num = 3\n",
        "\n",
        "class LinearRegression(object):\n",
        "    def __init__(self):\n",
        "        self.w = np.random.rand(feature_num)\n",
        "        self.b = np.random.rand(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = np.matmul(x, self.w) + self.b\n",
        "        return y_pred\n",
        "    \n",
        "    def normalize(self, x):\n",
        "        for feature in x.T:\n",
        "            fmean = np.mean(feature)\n",
        "            frange = np.amax(feature) - np.amin(feature)\n",
        "            feature -= fmean\n",
        "            feature /= frange\n",
        "        return x\n",
        "\n",
        "    def cost_function(self, x, y_true):\n",
        "        n = len(y_true)\n",
        "        y_pred = self.forward(x)\n",
        "        sq_error = (y_pred - y_true) ** 2\n",
        "        return 1.0 / (2 * n) * sq_error.sum()\n",
        "    \n",
        "    def compute_gradients(self, x, y_true):\n",
        "        y_pred = self.forward(x)\n",
        "        difference = y_pred - y_true\n",
        "        gradient_b = np.mean(difference)\n",
        "        gradients_w = np.matmul(x.transpose(), difference)\n",
        "        gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
        "\n",
        "        return gradients_w, gradient_b\n",
        "    \n",
        "    def update_weightes(self, x, y_true, learning_rate):\n",
        "        gradients_w, gradient_b = self.compute_gradients(x, y_true)\n",
        "        self.w = self.w - learning_rate * gradients_w\n",
        "        self.b = self.b - learning_rate * gradient_b\n",
        "    \n",
        "    def train(self, x, y):\n",
        "        epoch_num = 1000\n",
        "        learning_rate = 0.01\n",
        "        for epoch in range(epoch_num):\n",
        "            model.update_weightes(x, y, learning_rate)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "N = 1000\n",
        "# y = 1 * x1 - 2 * x2 + 3 * x3 + 12\n",
        "def gen_data(x):\n",
        "    y = []\n",
        "    for i in range(N):\n",
        "        y.append(x[i][0] - 2 * x[i][1] + 3 * x[i][2] + 12)\n",
        "    return np.array(y)\n",
        "\n",
        "x = np.random.rand(N, 3)\n",
        "y = gen_data(x)\n",
        "\n",
        "x = model.normalize(x)\n",
        "model.train(x, y)\n",
        "print(model.w, model.b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYPydWTvsuW9",
        "outputId": "7665ba68-e7bd-4da6-a310-128822b9fbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.99828682 -1.99113219  2.99342082] [-268782.10094641]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "### Introduction\n",
        "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
        " \n",
        "### **Sigmoid Function**\n",
        "In order to map predicted values to probabilities, we use the sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n",
        "$$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "```python\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "```\n",
        "\n",
        "### **Cost Function**\n",
        "Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for $y=1$ and one for $y=0$.\n",
        "$$L = \\frac{1}{m}(-y^{T}log(y\\_pred) - (1-y)^{T}log(1-y\\_pred))$$"
      ],
      "metadata": {
        "id": "0lpiTbEENz-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "class LogisticRegression(object):\n",
        "    def __init__(self):\n",
        "        self.w = np.random.rand(feature_num)\n",
        "        self.b = np.random.rand(1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y_pred = np.matmul(x, self.w) + self.b\n",
        "        y_pred = sigmoid(y_pred)\n",
        "        return y_pred\n",
        "    \n",
        "    def normalize(self, x):\n",
        "        for feature in x.T:\n",
        "            fmean = np.mean(feature)\n",
        "            frange = np.amax(feature) - np.amin(feature)\n",
        "            feature -= fmean\n",
        "            feature /= frange\n",
        "        return x\n",
        "\n",
        "    def cross_entropy(self, x, y_true):\n",
        "        m = len(y_true)\n",
        "        y_pred = self.forward(x)\n",
        "        class1_cost = -y_true * np.log(y_pred)\n",
        "        class2_cost = (1 - y_true) * np.log(1 - y_pred)\n",
        "        cost = class1_cost - class2_cost\n",
        "        cost = cost.sum() / m\n",
        "        return cost\n",
        "    \n",
        "    def compute_gradients(self, x, y_true):\n",
        "        y_pred = self.forward(x)\n",
        "        difference = y_pred - y_true\n",
        "        gradient_b = np.mean(difference)\n",
        "        gradients_w = np.matmul(x.transpose(), difference)\n",
        "        gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
        "\n",
        "        return gradients_w, gradient_b\n",
        "    \n",
        "    def update_weightes(self, x, y_true, learning_rate):\n",
        "        gradients_w, gradient_b = self.compute_gradients(x, y_true)\n",
        "        self.w = self.w - learning_rate * gradients_w\n",
        "        self.b = self.b - learning_rate * gradient_b\n",
        "    \n",
        "    def train(self, x, y):\n",
        "        epoch_num = 1000\n",
        "        learning_rate = 0.01\n",
        "        for epoch in range(epoch_num):\n",
        "            model.update_weightes(x, y, learning_rate)\n",
        "    \n",
        "    def predict(self, x, y):\n",
        "        print(y)\n",
        "        print(self.forward(x))\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "N = 1000\n",
        "# y = 1 * x1 - 2 * x2 + 3 * x3 - 1\n",
        "def gen_data(x):\n",
        "    y = []\n",
        "    for i in range(N):\n",
        "        y_true = x[i][0] - 2 * x[i][1] + 3 * x[i][2] - 1\n",
        "        if y_true > 0:\n",
        "            y.append(1)\n",
        "        else:\n",
        "            y.append(0)\n",
        "    return np.array(y)\n",
        "\n",
        "x = np.random.rand(N, 3)\n",
        "y = gen_data(x)\n",
        "\n",
        "x = model.normalize(x)\n",
        "model.train(x, y)\n",
        "print(model.w, model.b)\n",
        "model.predict(x[:10], y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPeDIxp3tJEJ",
        "outputId": "14e94ec5-f017-433c-a96b-4bbed7d0ec7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 10.44756978 -20.78139095  31.64716938] [0.53287193]\n",
            "[0 0 1 0 0 1 1 1 1 1]\n",
            "[5.48030739e-01 1.92401937e-05 1.00000000e+00 8.33032784e-02\n",
            " 3.62622457e-10 9.99999998e-01 9.99941493e-01 9.99862098e-01\n",
            " 9.99999097e-01 9.99970778e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glossary\n",
        "[https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html](https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html)"
      ],
      "metadata": {
        "id": "OHGmEmUFYZyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FRGLjekTsxLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zjF-jURWmoLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fVa5gq8NtxP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Desent\n",
        "\n",
        "## [Adam](https://machinelearningmastery.com/adam-optimization-from-scratch)\n",
        "> **TODO**: 先实现一个basic dnn，可以使用手写数字的例子，然后把所有的basic tech都用上。all from scratch\n",
        "\n",
        "The Adaptive Movement Estimation algorithm, or Adam for short, is an extension to gradient descent and a natural successor to techniques like AdaGrad and RMSProp that automatically adapts a learning rate for each input variable for the objective function and further smooths the search process by using an exponentially decreasing moving average of the gradient to make updates to variables.\n",
        "+ Gradient descent is an optimization algorithm that uses the gradient of the objective function to navigate the search space.\n",
        "+ Gradient descent can be updated to use an automatically adaptive step size for each input variable using a decaying average of partial derivatives, called Adam.\n",
        "+ How to implement the Adam optimization algorithm from scratch and apply it to an objective function and evaluate the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "EwbcysqlmonH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DeepNeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        self.input_layer_size = 128\n",
        "        self.hidden_layer_size = 256\n",
        "        self.output_layer_size = 2\n",
        "        self.init_weights()\n",
        "        self.init_bias()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        self.w1 = np.random.randn(self.input_layer_size, self.hidden_layer_size) * \\\n",
        "                np.sqrt(2.0 / self.input_layer_size)\n",
        "        self.w2 = np.random.randn(self.hidden_layer_size, self.output_layer_size) * \\\n",
        "                np.sqrt(2.0 / self.hidden_layer_size)\n",
        "    \n",
        "    def init_bias(self):\n",
        "        self.b1 = np.full((1, self.hidden_layer_size), 0.1)\n",
        "        self.b2 = np.full((1, self.output_layer_size), 0.1)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_prime(self, x):\n",
        "        x[x < 0] = 0\n",
        "        x[x > 0] = 1\n",
        "        return x\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, x):\n",
        "         # Hidden layer\n",
        "        x = np.dot(x, self.w1) + self.b1\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = np.dot(x, self.w2) + self.b2\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "    \n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"\n",
        "        反向传播的本质是利用chain rule求解整个function对w的导数\n",
        "        \"\"\"\n",
        "        y_pred = self.forward(x)\n",
        "        d = (y_pred - y) * self.relu_prime(y_pred)\n",
        "        print(loss)\n",
        "\n",
        "dnn = DeepNeuralNetwork()\n",
        "x = np.random.randn(dnn.input_layer_size)\n",
        "y = np.array([1, 0])\n",
        "print(dnn.forward(x))\n",
        "dnn.backprop(x, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "OzDmU5vn5vIo",
        "outputId": "fdf19807-b3f8-429d-ca6c-9ea1c28484c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75108166 0.        ]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bafe5ad66832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-bafe5ad66832>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aaZHbRemqPRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZL0Vujx8788n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}