{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommendation System.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2mH2gv3XpF4LQQ1GVijEo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phonism/notes/blob/master/Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation System"
      ],
      "metadata": {
        "id": "TnV4a_Kq8T8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some equation\n",
        "## Softmax && Cross Entropy Loss\n",
        "$$Softmax(x_{i})=S(x_i)=\\frac{e^{x_{i}}}{\\sum_{k}e^{x_{k}}}$$\n",
        "$$CrossEntropy = -\\sum_{k}y_{k} lnS(x_{k}) = -lnS(x_{i})$$\n",
        "$$CrossEntropy' = S(z) - y$$\n",
        "## Sigmoid\n",
        "$$Sigmoid(x)=\\frac{1}{1+e^(-x)}$$\n",
        "$$Sigmoid'(x)=Sigmoid(x)(1 - Sigmoid(x))$$\n",
        "## Auc (How to calculate AUC)\n"
      ],
      "metadata": {
        "id": "Rtj8Kz9q9Hh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization?\n",
        "Regularization will prevent overfitting. When we have a lot of features (or very deep model)\n",
        "\n",
        "+ work by adding a penalty or shrinkage term called a regularization term to the loss function\n",
        "+ l1 regularzaion adds the \"absolute value of magnitude\" of the coefficient as a penalty term to the loss function.\n",
        "+ l2 regularzaion adds the “squared magnitude” of the coefficient as the penalty term to the loss function\n"
      ],
      "metadata": {
        "id": "7ngwX_piAg6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative Filtering\n",
        "To address some of the limitations of content-based filtering, collaborative filtering uses similarities between users and items simultaneously to provide recommendations. This allows for serendipitous recommendations; that is, collaborative filtering models can recommend an item to user A based on the interests of a similar user B. Furthermore, the embeddings can be learned automatically, without relying on hand-engineering of features.\n",
        "+ Advantages: No domain knowledge necessary, Great starting point\n",
        "+ Disadvantages: Cannot handle fresh items, Hard to include side features for query/item\n",
        "## Item-CF\n",
        "based on the similarity between items calculated using people's ratings of those items (users who bought x also bought y)\n",
        "## User-CF\n",
        "based on the similarity between users calculated using same item bought by different user (users who's interset is similar)\n"
      ],
      "metadata": {
        "id": "fagx5I6KQ4ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "## Introduction\n",
        "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.\n",
        " \n",
        "## **Sigmoid Function**\n",
        "In order to map predicted values to probabilities, we use the sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n",
        "$$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "```python\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "```\n",
        "\n",
        "## **Cost Function**\n",
        "Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for $y=1$ and one for $y=0$.\n",
        "$$L = \\frac{1}{m}(-y^{T}log(y\\_pred) - (1-y)^{T}log(1-y\\_pred))$$"
      ],
      "metadata": {
        "id": "dXTGl_qiVCaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factorization Machines\n",
        "Factorization Machine type algorithms are a combination of linear regression and matrix factorization, the cool idea behind this type of algorithm is it aims model interactions between features (a.k.a attributes, explanatory variables) using factorized parameters. By doing so it has the ability to estimate all interactions between features even with extremely sparse data.\n",
        "\n"
      ],
      "metadata": {
        "id": "7dAnoY7ZVt56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class FeaturesLinear(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
        "\n",
        "\n",
        "class FeaturesEmbedding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
        "        return self.embedding(x)\n",
        "\n",
        "class FactorizationMachine(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, reduce_sum=True):\n",
        "        super().__init__()\n",
        "        self.reduce_sum = reduce_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
        "        \"\"\"\n",
        "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
        "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
        "        ix = square_of_sum - sum_of_square\n",
        "        if self.reduce_sum:\n",
        "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
        "        return 0.5 * ix\n",
        "\n",
        "class FactorizationMachineModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A pytorch implementation of Factorization Machine.\n",
        "    Reference:\n",
        "        S Rendle, Factorization Machines, 2010.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, field_dims, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
        "        self.linear = FeaturesLinear(field_dims)\n",
        "        self.fm = FactorizationMachine(reduce_sum=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
        "        \"\"\"\n",
        "        x = self.linear(x) + self.fm(self.embedding(x))\n",
        "        return torch.sigmoid(x.squeeze(1))"
      ],
      "metadata": {
        "id": "dy30XKgnXQr2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GBDT + LR\n",
        "use gbdt to generate embedding features"
      ],
      "metadata": {
        "id": "Gd_Z0AgYa4Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wide & Deep\n",
        "Wide and Deep Learning Model has two main components.\n",
        "+ Wide: Memorization, Memorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. a linear model with a wide set of cross-product feature transformations.\n",
        "+ Deep: Generalization, Generalization, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past. a deep feed-forward neural network. each feature has it's embeddings.\n",
        "\n",
        "## DeepFM\n",
        "Wide part using facterization machines"
      ],
      "metadata": {
        "id": "QLKHMmo1bdPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abacus\n",
        "A framework for large-scale discrete DNN models based on parameter servers\n",
        "## Architecture\n",
        "+ Support distributed multi-machine training based on mpi. And it have a single master node\n",
        "+ Network communication with zeromq\n",
        "+ After the sample is shuffled, it is distributed to multiple machines and trained in parallel\n",
        "+ The training node itself acts as a parameter server, providing distributed parameter services (large and small models), The large model is a sparse table that stores hundreds of billions of feasign, and it's show, click, embedding, etc. The small model is the DNN model, which stores the DNN parameters. \n",
        "\n",
        "## Structure\n",
        "## Training Process\n",
        "The traning process have two stages: join and update\n",
        "+ in join stage: query the sparse table according to the input feasign to obtain show, ctr, lr, emb. and will update the join dnn network parameters, and estimate online.\n",
        "+ in update stage: query the sparse table according to the input feasign to obtain lr, emb. And will update the update dnn network parameters and the sparse table\n",
        "\n",
        "why?\n",
        "+ The embedding training speed is slow, and the expected changes are small. When new data comes, train the join first, without updating the embedding. The model can quickly learn the latest data distribution and take effect online\n",
        "+ It can alleviate the over-fitting situation, because the online prediction models are all embedding at time T-1 and dnn at time T, and the information of the current sample is not used.\n",
        "+ There will be strong features such as feasign's show and click in the join stage, but not in the update stage. If we trained the model end2end, these strong features may lead to biased embedding learning\n",
        "+ The join stage has strong features, it will learn very quickly, and the importance of each slot is clear. In the update stage, it can help the embedding converge in the correct direction."
      ],
      "metadata": {
        "id": "9rmaEU5Ucq-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias\n",
        "## Cold-start\n",
        "+ Given a new item not seen in training, if the system has a few interactions with users, then the system can easily compute an embedding for this item without having to retrain the whole model. The item embedding can be the average of user embeddings.\n",
        "+ Heuristics to generate embeddings of fresh items. If the system does not have interactions, the system can approximate its embedding by averaging the embeddings of items from the same category, from the same uploader (in YouTube), and so on."
      ],
      "metadata": {
        "id": "B4BFWflrTD8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Egt1k7Sh9kmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}